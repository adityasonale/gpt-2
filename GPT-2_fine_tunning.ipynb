{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Omen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Omen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "import os\n",
    "import docx\n",
    "\n",
    "\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function ot read files from pdf\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    with open(file_path,\"rb\") as file:\n",
    "        pdf_reader = PdfReader(file)\n",
    "        text = \"\"\n",
    "\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            text += pdf_reader.pages[page_num].extract_text()\n",
    "    return text\n",
    "\n",
    "def read_word(file_path):\n",
    "    docs = docx.Document(file_path)\n",
    "    text = \"\"\n",
    "    for paragraph in docs.paragraphs:\n",
    "        text += paragraph.text + \"\\n\"\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_pdf(r\"C:\\Users\\Omen\\OneDrive\\Desktop\\Linear Regression.pdf\")\n",
    "\n",
    "data = re.sub(r'\\n+', '\\n', data).strip() # remove excess newline characters\n",
    "\n",
    "with open(r\"data.txt\", \"w\") as f:\n",
    "    f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path, tokenizer, block_size = 128):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=block_size\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_collator(tokenizer, mlm = False):\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=mlm\n",
    "    )\n",
    "\n",
    "    return data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_file_path,model_name,output_dir,overwrite_output_dir, per_device_train_batch_size,num_train_epochs,save_steps):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    train_dataset = load_dataset(train_file_path,tokenizer=tokenizer)\n",
    "    data_collator = load_data_collator(tokenizer=tokenizer)\n",
    "\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=overwrite_output_dir,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_path = r\"data.txt\"\n",
    "model_name = 'gpt2'\n",
    "output_dir = \"./saved_model\"\n",
    "overwrite_output_dir = False\n",
    "per_device_batch_size = 8\n",
    "num_train_epochs = 100\n",
    "save_steps = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Omen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:20<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 20.5972, 'train_samples_per_second': 29.13, 'train_steps_per_second': 4.855, 'train_loss': 0.632500343322754, 'epoch': 100.0}\n"
     ]
    }
   ],
   "source": [
    "train(train_file_path=training_file_path,\n",
    "      model_name=model_name,\n",
    "      output_dir=output_dir,\n",
    "      overwrite_output_dir=overwrite_output_dir,\n",
    "      per_device_train_batch_size=per_device_batch_size,\n",
    "      num_train_epochs=num_train_epochs,\n",
    "      save_steps=save_steps\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    return model\n",
    "\n",
    "def load_tokenizer(tokenizer_path):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    "    return tokenizer\n",
    "\n",
    "def generate_text(model_path, sequence, max_length):\n",
    "    \n",
    "    model = load_model(model_path)\n",
    "    tokenizer = load_tokenizer(model_path)\n",
    "    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n",
    "    final_outputs = model.generate(\n",
    "        ids,\n",
    "        do_sample=True,\n",
    "        max_length=max_length,\n",
    "        pad_token_id=model.config.eos_token_id,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q] What is Linear Regression? Linear regression is a machine learning library for machine learning that makes predictions using sparse training sets. Linear regression is a machine learning library for linear regression that learns by fitting the data with gradient descent. Linear regression is\n"
     ]
    }
   ],
   "source": [
    "model_path = r\"D:\\vs code\\python\\DeepLearning\\LLMs\\GPT-2\\saved_model\"\n",
    "sequence1 = \"[Q] What is Linear Regression?\"\n",
    "max_len = 50\n",
    "generate_text(model_path, sequence1, max_len) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
